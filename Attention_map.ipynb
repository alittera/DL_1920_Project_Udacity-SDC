{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import string\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import ipynb.fs.full.MyDataset as MyDataset\n",
    "import ipynb.fs.full.Models as Models\n",
    "import ipynb.fs.full.Utils as Utils\n",
    "import ipynb.fs.full.RMSELoss as Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntransform = transforms.Compose(\\n    [transforms.RandomResizedCrop(224),\\n        transforms.RandomHorizontalFlip(),\\n        transforms.ToTensor()])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        #transforms.Resize(size=400),\n",
    "        #transforms.RandomRotation(degrees=2),\n",
    "        #transforms.ColorJitter(),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "transform = image_transforms[\"train\"]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_resized=True\n",
    "if pre_resized:\n",
    "    csv_file = \"data/Ch2_002_resized/interpolated.csv\"\n",
    "    directory = \"data/Ch2_002_resized\"\n",
    "else:\n",
    "    csv_file = \"data/Ch2_002/interpolated.csv\"\n",
    "    directory = \"data/Ch2_002\"  \n",
    "\n",
    "dataset = MyDataset.MyDataset(csv_file, directory, transform = trans, enable_cache=False, reduce_near_zero_samples=False)\n",
    "\n",
    "\n",
    "a,b,c = int(0.5*len(dataset)),int(0.25*len(dataset)),int(0.25*len(dataset))\n",
    "m = len(dataset) - (a+b+c)\n",
    "\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [a+m,b,c])\n",
    "testloader = DataLoader(dataset, batch_size=1, shuffle=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpu: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet2(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(3, 3))\n",
       "  (bnm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(6, 8, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "  (bnm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "  (bnm5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool4): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (relu4): ReLU(inplace=True)\n",
       "  (relu5): ReLU(inplace=True)\n",
       "  (f): Flatten()\n",
       "  (fc1): Linear(in_features=4032, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (fc4): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc5): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Models.ConvNet2()\n",
    "model.load_state_dict(torch.load(\"save_model/mytraining.pt\"))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Gpu:\",use_cuda)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    " \n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    model.to(device)\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import PIL\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "class CamExtractor():\n",
    "    \"\"\"\n",
    "        Extracts cam features from the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad.cpu()\n",
    "        \n",
    "\n",
    "    def forward_pass_on_convolutions(self, x):\n",
    "        \"\"\"\n",
    "            Does a forward pass on convolutions, hooks the function at given layer\n",
    "        \"\"\"\n",
    "        x =  self.model.conv1(x)\n",
    "        x =  self.model.bnm1(x)\n",
    "        x =  self.model.relu1(x)\n",
    "        x =  self.model.pool3(x)\n",
    "\n",
    "        x =  self.model.conv2(x)\n",
    "        x =  self.model.bnm2(x)\n",
    "        x =  self.model.relu2(x)\n",
    "        x =  self.model.pool2(x)\n",
    "        \n",
    "        x =  self.model.conv3(x)\n",
    "        x =  self.model.bnm3(x)\n",
    "        x =  self.model.relu3(x)\n",
    "        x =  self.model.pool2(x)\n",
    "        \n",
    "        x =  self.model.conv4(x)\n",
    "        x =  self.model.bnm4(x)\n",
    "        x =  self.model.relu4(x)\n",
    "        x =  self.model.pool2(x)\n",
    "        \n",
    "        \n",
    "        x =  self.model.conv5(x)\n",
    "        x =  self.model.bnm5(x)\n",
    "        x =  self.model.relu5(x)\n",
    "        \n",
    "        x.register_hook(self.save_gradient)\n",
    "        conv_output = x.cpu()  # Save the convolution output on that layer\n",
    "\n",
    "        x = x.view(x.shape[0],fl_1_input_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.model.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.model.dropout1(x)\n",
    "        \n",
    "        x = self.model.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.model.dropout2(x)\n",
    "        \n",
    "        #print(\"fc2\", x.shape)\n",
    "        \n",
    "        x = self.model.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.model.dropout3(x)\n",
    "        \n",
    "        x = self.model.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.dropout4(x)\n",
    "        \n",
    "        x = self.model.fc5(x)\n",
    "        \n",
    "        return conv_output, x\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "            Does a full forward pass on the model\n",
    "        \"\"\"\n",
    "        # Forward pass on the convolutions\n",
    "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
    "\n",
    "        return conv_output, x\n",
    "\n",
    "\n",
    "class GradCam():\n",
    "    \"\"\"\n",
    "        Produces class activation map\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Define extractor\n",
    "        self.extractor = CamExtractor(self.model, target_layer)\n",
    "\n",
    "    def generate_cam(self, input_image, angle=None):\n",
    "        # Full forward pass\n",
    "        # conv_output is the output of convolutions at specified layer\n",
    "        # model_output is the final output of the model (1, 1000)\n",
    "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
    "        if angle is None:\n",
    "            print(\"No angle given\")\n",
    "\n",
    "        # Zero grads\n",
    "        self.model.fc5.zero_grad()\n",
    "        # self.model.classifier.zero_grad()\n",
    "        # Backward pass with specified target\n",
    "        model_output.backward(gradient=angle.unsqueeze(1), retain_graph=True)\n",
    "        \n",
    "        \n",
    "        # Get hooked gradients\n",
    "        guided_gradients = self.extractor.gradients.data.numpy()[0]\n",
    "        # Get convolution outputs\n",
    "        target = conv_output.data.numpy()[0]\n",
    "        # Get weights from gradients\n",
    "        # Take averages for each gradient\n",
    "        weights = np.mean(guided_gradients, axis=(1, 2))\n",
    "        # Create empty numpy array for cam\n",
    "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
    "        # Multiply each weight with its conv output and then, sum\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "        cam = cv2.resize(cam, (640, 480))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) -\n",
    "                                     np.min(cam))  # Normalize between 0-1\n",
    "        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "        return cam\n",
    "\n",
    "def save_class_activation_on_image(org_img, activation_map, file_name):\n",
    "    \"\"\"\n",
    "        Saves cam activation map and activation map on the original image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): activation map (grayscale) 0-255\n",
    "        file_name (str): File name of the exported image\n",
    "    \"\"\"\n",
    "    if not os.path.exists('./heatmap'):\n",
    "        os.makedirs('./heatmap')\n",
    "    # Grayscale activation map\n",
    "    #path_to_file = os.path.join('./results', file_name + '_Cam_Grayscale.jpg')\n",
    "    #cv2.imwrite(path_to_file, activation_map)\n",
    "    # Heatmap of activation map\n",
    "    activation_heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_HSV)\n",
    "    #path_to_file = os.path.join('./results', file_name + '_Cam_Heatmap.jpg')\n",
    "    #cv2.imwrite(path_to_file, activation_heatmap)\n",
    "    \n",
    "    # Heatmap on picture\n",
    "    #org_img = cv2.resize(org_img, (224, 224))\n",
    "    img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n",
    "    img_with_heatmap = img_with_heatmap / np.max(img_with_heatmap)\n",
    "    path_to_file = os.path.join('./heatmap', file_name )\n",
    "    cv2.imwrite(path_to_file, np.uint8(255 * img_with_heatmap))\n",
    "    #cv2.imshow(file_name,np.uint8(255 * img_with_heatmap))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting heatmap generation\n",
      "999 Grad cam completed\n",
      "1998 Grad cam completed\n",
      "2997 Grad cam completed\n",
      "3996 Grad cam completed\n",
      "4995 Grad cam completed\n",
      "5994 Grad cam completed\n",
      "6993 Grad cam completed\n",
      "7992 Grad cam completed\n",
      "8991 Grad cam completed\n",
      "9990 Grad cam completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:141: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10989 Grad cam completed\n",
      "11988 Grad cam completed\n",
      "12987 Grad cam completed\n",
      "13986 Grad cam completed\n",
      "14985 Grad cam completed\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print (\"starting heatmap generation\")\n",
    "for data in testloader:\n",
    "\n",
    "\n",
    "    path = data['Path'][0]\n",
    "\n",
    "    if \"center\" in path:\n",
    "        i+=1\n",
    "        \n",
    "        if i > 9998:\n",
    "            \n",
    "            inputs = data['X']\n",
    "            angles = data['y']\n",
    "\n",
    "            path_ = path.replace(\"_resized\", \"\")\n",
    "            name = path.split(\"/\")[-1]\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            angles = angles.to(device)\n",
    "            outputs = torch.squeeze(model(inputs))\n",
    "            # Open CV preporcessing\n",
    "            image = cv2.imread(path)\n",
    "            \n",
    "\n",
    "            grad_cam = GradCam(model, target_layer='')\n",
    "            # Generate cam mask\n",
    "            cam = grad_cam.generate_cam(inputs, angles)\n",
    "\n",
    "            #print(cam)\n",
    "            # Save mask\n",
    "            image_ = cv2.imread(path_)\n",
    "            save_class_activation_on_image(image_, cam, name)\n",
    "            \n",
    "        if i%999==0:\n",
    "            print(i,'Grad cam completed')\n",
    "\n",
    "        if i%15099==0:\n",
    "            break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
